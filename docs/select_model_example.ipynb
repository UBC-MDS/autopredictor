{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "\n",
    "To use `autopredictor` in a project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0\n"
     ]
    }
   ],
   "source": [
    "import autopredictor\n",
    "\n",
    "print(autopredictor.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autopredictor.fit import fit\n",
    "from autopredictor.show_all import show_all\n",
    "from autopredictor.bestscore import display_best_score\n",
    "from autopredictor.select_model import select_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-life scenarios, like in healthcare, the process of selecting appropriate models, determining relevant metrics, scrutinizing specific models, and ultimately choosing the most suitable model is not only time-consuming but also critical in influencing decision-making and patient outcomes. Our package, autopredictor, is meticulously designed to streamline these aspects of machine learning for continuous data, thereby significantly accelerating the workflow. This acceleration is particularly beneficial for healthcare professionals, data scientists, or researchers, as it allows them to allocate more time towards insightful data interpretation and strategic decision-making. The diabetes dataset showcased in this example serves as a representative sample of real-world health data. The methodology employed in training, evaluating, and selecting models closely parallels the procedural work observed in actual research settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression trained.\n",
      "Linear Regression (L1) trained.\n",
      "Linear Regression (L2) trained.\n",
      "Linear Support Vector Machine trained.\n",
      "Support Vector Machine trained.\n",
      "Decision Tree trained.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zeily/miniconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest trained.\n",
      "Gradient Boosting trained.\n",
      "AdaBoost trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Linear Regression': {'Mean Absolute Error': 42.79409467959994,\n",
       "   'Mean Absolute Percentage Error': 0.3749982636756113,\n",
       "   'R2 Score': 0.4526027629719195,\n",
       "   'Mean Squared Error': 2900.1936284934814,\n",
       "   'Root Mean Squared Error': 53.85344583676593},\n",
       "  'Linear Regression (L1)': {'Mean Absolute Error': 49.73032753662261,\n",
       "   'Mean Absolute Percentage Error': 0.47112563453406076,\n",
       "   'R2 Score': 0.3575918767219115,\n",
       "   'Mean Squared Error': 3403.5757216070733,\n",
       "   'Root Mean Squared Error': 58.340172450954185},\n",
       "  'Linear Regression (L2)': {'Mean Absolute Error': 46.13885766697452,\n",
       "   'Mean Absolute Percentage Error': 0.42569291627271477,\n",
       "   'R2 Score': 0.41915292635986556,\n",
       "   'Mean Squared Error': 3077.4159388272296,\n",
       "   'Root Mean Squared Error': 55.47446204180109},\n",
       "  'Linear Support Vector Machine': {'Mean Absolute Error': 63.365412233872696,\n",
       "   'Mean Absolute Percentage Error': 0.43105253685704287,\n",
       "   'R2 Score': -0.27877871793826237,\n",
       "   'Mean Squared Error': 6775.163700410422,\n",
       "   'Root Mean Squared Error': 82.31138256894013},\n",
       "  'Support Vector Machine': {'Mean Absolute Error': 56.02372412801096,\n",
       "   'Mean Absolute Percentage Error': 0.49028421404764305,\n",
       "   'R2 Score': 0.18211365770500287,\n",
       "   'Mean Squared Error': 4333.285954518086,\n",
       "   'Root Mean Squared Error': 65.82769899151941},\n",
       "  'Decision Tree': {'Mean Absolute Error': 54.0,\n",
       "   'Mean Absolute Percentage Error': 0.44027960780382436,\n",
       "   'R2 Score': 0.07202746179943798,\n",
       "   'Mean Squared Error': 4916.539325842697,\n",
       "   'Root Mean Squared Error': 70.11803851964697},\n",
       "  'Random Forest': {'Mean Absolute Error': 44.17764044943821,\n",
       "   'Mean Absolute Percentage Error': 0.394964380582144,\n",
       "   'R2 Score': 0.43796587416534927,\n",
       "   'Mean Squared Error': 2977.742086516854,\n",
       "   'Root Mean Squared Error': 54.56869144955608},\n",
       "  'Gradient Boosting': {'Mean Absolute Error': 44.678309800527295,\n",
       "   'Mean Absolute Percentage Error': 0.40097307104175534,\n",
       "   'R2 Score': 0.4477348698859227,\n",
       "   'Mean Squared Error': 2925.984464758647,\n",
       "   'Root Mean Squared Error': 54.09236974619107},\n",
       "  'AdaBoost': {'Mean Absolute Error': 44.90474853019659,\n",
       "   'Mean Absolute Percentage Error': 0.42529757580414573,\n",
       "   'R2 Score': 0.43060963825482845,\n",
       "   'Mean Squared Error': 3016.716540668681,\n",
       "   'Root Mean Squared Error': 54.92464420156658}},\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = fit(X_train, X_test, y_train, y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHOW ALL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input should be of dictionary type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m score_df \u001b[38;5;241m=\u001b[39m \u001b[43mshow_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m score_df\n",
      "File \u001b[0;32m~/MDS_git/524/autopredictor/src/autopredictor/show_all.py:53\u001b[0m, in \u001b[0;36mshow_all\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Check if result is of type dictionary\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput should be of dictionary type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Check if result is an empty dictionary\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m==\u001b[39m {}:\n",
      "\u001b[0;31mTypeError\u001b[0m: Input should be of dictionary type."
     ]
    }
   ],
   "source": [
    "score_df = show_all(model_scores)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL WITH THE BEST SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m display_best_score(\u001b[43mscore_df\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m score_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score_df' is not defined"
     ]
    }
   ],
   "source": [
    "best_model = display_best_score(score_df, 'MAE')\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the models and evaluating their performance, the select_model function allows the user to select a specific model and view its performance metrics. It ensures that the inputs are of the correct types and that the specified model is present in the results. If the model is found, it returns the performance metrics for that model; otherwise, it provides a list of available models. This function is particularly useful for zooming in on a specific model's performance or for retrieving the performance of the best model based on a particular metric.\n",
    "\n",
    "In the context of a diabetes dataset, for instance, if an analyst suspects that a certain model, like a Random Forest Regressor, might be particularly well-suited to handling the complexities of diabetes data (due to its ability to model non-linear relationships and interactions between variables), the select_model function allows them to isolate and closely examine the performance of just this model. This is especially useful in situations where a multitude of models have been trained and evaluated, and there's a need to drill down into the specifics of one model without getting overwhelmed by the broader data.\n",
    "\n",
    "Expanding beyond healthcare, this function has broad applicability in various fields. For example, in finance, an analyst might want to specifically evaluate the performance of a particular model in predicting stock prices or market trends. Similarly, in environmental science, a researcher could use this function to singularly assess a model's accuracy in forecasting climate patterns or pollution levels.\n",
    "\n",
    "The ability to selectively examine a model is crucial when comparing models that might have different strengths and weaknesses depending on the context. This targeted approach enables a more thoughtful and focused analysis, allowing analysts to make more informed decisions about which model to deploy based on specific criteria relevant to their field or problem at hand. It's a tool that enhances precision in model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m selected_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdaBoost\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m AdaBoost_scores \u001b[38;5;241m=\u001b[39m select_model(\u001b[43mscore_df\u001b[49m, selected_model_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score_df' is not defined"
     ]
    }
   ],
   "source": [
    "selected_model_name = 'AdaBoost'\n",
    "AdaBoost_scores = select_model(score_df, selected_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_name = 'Linear Regression'\n",
    "LR_scores = select_model(score_df, selected_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_name = 'Random Forest'\n",
    "RF_scores = select_model(score_df, selected_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECT MODEL ADDITIONAL FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The select_model function is not only beneficial for focusing on a specific model's performance but also serves as a useful tool for verifying the presence of a model within the dataset. When an analyst specifies a model, the function checks if that model is included in the DataFrame's index. If the model is not found, the function doesn't just stop at returning an error message; it goes a step further by providing a list of the models that are included. This feature is particularly helpful in multiple ways:\n",
    "\n",
    "- **Error Prevention and Troubleshooting:** It helps in preventing errors that might occur from typos or incorrect model names. By returning an error message and a list of included models, it guides the user towards the correct model names, making the troubleshooting process more intuitive and less time-consuming.\n",
    "\n",
    "- **Model Inventory Check:** It essentially acts as a quick inventory check, allowing users to confirm which models have been trained and evaluated. This is especially useful in collaborative environments where multiple team members might be working on the same dataset but focusing on different models. It ensures that everyone is aware of the models that are already included in the analysis, helping to avoid redundant work.\n",
    "\n",
    "- **Informed Decision Making:** By providing a list of available models, it aids in informed decision-making. Analysts can quickly scan through the available models and decide which ones to focus on based on their specific criteria or hypothesis, without having to look through the entire DataFrame.\n",
    "\n",
    "For example, in a real-life scenario, an environmental scientist analyzing a dataset on air quality might be interested in examining a specific model's ability to predict pollution levels. If they aren't sure whether the model has been included in the analysis, they can use the select_model function. If the model is not found, the function's feedback not only informs them of this but also shows which models are available, allowing the scientist to make an informed decision on whether to proceed with an available trained model or to train and evaluate the model of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_name = 'Other Regressor'\n",
    "Other_Regressor_scores = select_model(score_df, selected_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrong Spelling\n",
    "selected_model_name = 'Rand Fore'\n",
    "RF_scores = select_model(score_df, selected_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our script presents a comprehensive framework for model selection within the realm of machine learning tasks. It facilitates the training and evaluation of a diverse array of models, provides a detailed array of performance metrics for thorough assessment, and ensures a clear, user-friendly presentation of results, aiding in the informed selection of the most effective model. The application of this script to the diabetes dataset underlines its relevance and adaptability to real-world data, underscoring its potential to be a valuable asset in healthcare analytics and various other domains where modeling is a key."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
